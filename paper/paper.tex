\documentclass{article}

\makeatletter
% Allow compiling either from the repo root (`pdflatex paper/paper.tex`) or
% from within `paper/` (`pdflatex paper.tex`).
\def\input@path{{./}{paper/}}
\makeatother

\input{definitions}
% If you need to pass options to natbib, use, e.g.:
%   \PassOptionsToPackage{numbers,compress}{natbib}
% before loading paper.

% ready for submission
\usepackage{neurips_2024}

% Standard packages
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % figures
\usepackage{enumitem}       % compact lists
\usepackage{multirow}       % tables
\usepackage{array}          % tables
\usepackage{siunitx}        % numbers in tables

% Local macros
\newcommand{\vectur}{\textsc{VecTur}}
\newcommand{\vecstur}{\textsc{VecSTur}}
\newcommand{\compgen}{\textsc{CompGen}}

\title{VecTur: Vector Turing Machines}

\author{%
  Ethan Hall \\
  \texttt{ethan.hall.phd@gmail.com} \\
}

\begin{document}
\maketitle

\begin{abstract}
We introduce \vectur{} (Vector Turing Machines), a differentiable, Turing-machine-inspired transition system that represents tape symbols, head position, and finite control as continuous vectors \citep{turing1936}.
Conceptually, \vectur{} continues a well-trodden line of differentiable memory machines (e.g., Neural Turing Machines and Differentiable Neural Computers) \citep{graves2014ntm,graves2016dnc}; our focus is a modern, sparse implementation that avoids dense content-based access at every step.
\vectur{} maintains a continuous ``head on a circle'' (\(S^1\)) and performs \emph{strictly local}, \(2k\)-sparse gather/scatter updates via interpolation, encouraging pointer-machine-style computation and mitigating the ``memory blurring'' failure mode of early dense-access NTMs.
\vectur{} also supports \emph{deep computation} \citep{dehghani2019universal} by explicitly iterating a learned transition map and using an ACT-style learned halting gate \citep{graves2016act} (parameterized by \(\kappa\)); we treat this halting parameterization as a practical heuristic rather than a core novelty claim.
We evaluate \vectur{} as a drop-in \emph{computational block} inside a Llama-style decoder-only language model \citep{touvron2023llama2,dubey2024llama3}, contrasting it with attention \citep{vaswani2017attention}, LSTM-style recurrence \citep{hochreiter1997lstm}, and differentiable external-memory baselines \citep{graves2014ntm,graves2016dnc}.
On small-to-medium open benchmarks for reasoning and language (GSM8K \citep{cobbe2021gsm8k}, ARC \citep{clark2018arc}, HellaSwag \citep{zellers2019hellaswag}, WikiText-103 \citep{merity2016wikitext}), we find \vectur{} improves algorithmic generalization \citep{kaiser2016neuralgpu,press2022trainshort} at fixed parameter budgets.
We additionally introduce \compgen{}---a synthetic program dataset stratified by \((T(n),S(n))\) complexity classes---as a utility benchmark for probing compute allocation.
Finally, we propose \vecstur{}, a stochastic extension that consumes random tape symbols and targets \emph{randomized algorithms} as a computational resource: \vecstur{} outperforms \vectur{} on randomized verification tasks (e.g., Freivalds-style matrix product verification \citep{freivalds1977}).
\end{abstract}

\section{Introduction}
Modern language models excel at pattern completion yet often struggle to reliably \emph{execute} long algorithmic computations, extrapolate beyond training lengths \citep{press2022trainshort}, or allocate variable compute per input \citep{graves2016act,dehghani2019universal}.
Several lines of work attempt to address these limitations by embedding algorithmic structure into neural systems, including external-memory architectures \citep{graves2014ntm,graves2016dnc} and adaptive computation mechanisms \citep{graves2016act}.

We propose \vectur{}, a vectorized analogue of a classical Turing machine \citep{turing1936} whose tape, head index, and finite control are represented as continuous vectors and updated by a learned transition map.
We do \emph{not} claim to have invented differentiable Turing machines; rather, we revisit this classic idea in a form that better matches modern LLM systems constraints.
Classic NTMs \citep{graves2014ntm} relied on dense, content-based attention over the entire memory at each step, which is \(O(N)\) in memory size and can introduce diffuse ``blurring'' updates.
In contrast, \vectur{} maintains a continuous head position on a circular tape (\(S^1\)) and enforces \emph{strictly local} access: each step reads and writes via interpolation over only \(2k\) tape indices (sparse gather/scatter), yielding per-step cost independent of tape length and an inductive bias closer to pointer machines \citep{vinyals2015pointer}.
For adaptive depth, \vectur{} includes an ACT-style halting mechanism \citep{graves2016act}; our particular \(\kappa\) parameterization is presented as a practical, input-conditioned control knob rather than a conceptual departure from ACT.

\paragraph{Why this helps for long context.}
Self-attention provides direct interaction between all token pairs but incurs \(O(N^2)\) compute in sequence length \(N\).
Linear-time alternatives avoid this quadratic cost, but typically do so by enforcing a \emph{linear progression} of computation through the sequence: in an LSTM, information from the past must be compressed into a fixed-size hidden state, and each new token updates that state once in time order.
More recent linear-time memory perspectives (e.g., the MIRAS viewpoint connecting retention, memorization, and online optimization \citep{behrouz2025miras}) likewise maintain and update memory in lockstep with the token stream.
In contrast, \vectur{} decouples interaction from all-pairs attention by introducing an explicit \emph{Turing index} (the head index \(I_t\)) whose motion over the tape is learned.
Across transition steps, head motion allows the model to create relationships between \emph{arbitrary} tokens by moving to (or copying from) the corresponding tape locations and combining them through the finite-control state, without computing a dense \(N\times N\) similarity matrix.
Crucially, this enables \emph{non-linear} token processing: the controller can revisit selected tokens or intermediate scratchpad cells multiple times, in an order determined by the evolving machine state, while leaving the rest of the tape unchanged by construction.
This selective persistence---deciding what to write to the tape, what to overwrite, and what to ignore---matches the operational structure of Turing-style computation (state + tape) more directly than recurrence with a single compressed memory vector.

We evaluate \vectur{} in a realistic regime by inserting it as a \emph{block} inside a Llama-style decoder-only macro architecture \citep{touvron2023llama2,dubey2024llama3}, replacing the standard attention+MLP block.
We compare against alternative blocks: (i) standard attention \citep{vaswani2017attention}, (ii) LSTM-style recurrence \citep{hochreiter1997lstm}, and (iii) differentiable external-memory controllers \citep{graves2014ntm,graves2016dnc}.
We focus on small-to-medium models (roughly \(10^8\) to \(10^9\) parameters) where architectural inductive bias can materially affect sample efficiency and extrapolation \citep{kaiser2016neuralgpu,press2022trainshort}.

We further introduce \compgen{}, a dataset of generated Python programs grouped into discrete complexity buckets \((T(n),S(n))\) such as \(O(n)\)/\(O(1)\), \(O(n\log n)\)/\(O(1)\), \(O(n^2)\)/\(O(1)\), and \(O(n^2)\)/\(O(n)\).
The goal is to probe whether a model can learn to \emph{compute} across increasing \(n\) by allocating more steps as needed, rather than memorizing only small \(n\).
Finally, we define \vecstur{}, which augments the input with stochastic symbols \(z\) to emulate randomized computation, and we propose a randomized evaluation suite where randomness yields asymptotic speedups \citep{freivalds1977,miller1976,rabin1980}. In particular, we form 
a data set of matrices $A,B,C\in \R^{n\times n}$ and a target matrix $D\in \R^{n\times n}$ such that $D = AB$ and $D = AC$ with probability $1/2$. We evaluate $\vectur{}$ and $\vecstur{}$ on this task, and show $\vecstur{}$ can exploit stochastic symbols to achieve asymptotic speedups.

\paragraph{Contributions.}
\begin{itemize}[leftmargin=*,nosep]
  \item We define \vectur{}, a Turing-style transition system with \emph{strictly local}, \(2k\)-sparse gather/scatter tape access (continuous head on \(S^1\)), addressing efficiency and ``memory blurring'' issues associated with dense-access NTMs \citep{graves2014ntm}.
  \item We present a plug-and-play integration of \vectur{} as a \emph{computational sub-layer} inside Llama-style decoder-only models, treating iterative algorithmic computation as a composable block rather than a separate retrieval module.
  \item We define \vecstur{} and a randomized computation evaluation suite, highlighting randomness as a computational resource (e.g., Freivalds-style verification \citep{freivalds1977}) rather than mere noise.
  \item We introduce \compgen{}, a synthetic program dataset labeled by time/space complexity class \((T(n),S(n))\), as a utility benchmark for extrapolation and compute-allocation probing.
\end{itemize}

\section{Related Work}
\paragraph{Sequence models and attention.}
Transformers \citep{vaswani2017attention} and their decoder-only variants power modern LLMs (e.g., GPT-3 \citep{brown2020gpt3} and Llama-family models \citep{touvron2023llama2,dubey2024llama3}).
Recurrent networks such as LSTMs \citep{hochreiter1997lstm} provide a different inductive bias for iterative computation but historically underperform attention-based models at scale on language modeling.

\paragraph{Differentiable memory and neural machines.}
Neural Turing Machines (NTMs) \citep{graves2014ntm} and Differentiable Neural Computers (DNCs) \citep{graves2016dnc} integrate external memory with differentiable read/write heads.
Our work shares the goal of improving algorithmic behavior, but emphasizes sparse, strictly local access (pointer-machine-style) and composable integration as a modern Transformer block; we include learned halting primarily as an ACT-style compute control mechanism.

\subsection{Remark: Neural Turing Machines vs.\ \vectur{}}
\label{sec:remark_ntm_vs_vectur}
Both NTMs \citep{graves2014ntm} and \vectur{} augment neural computation with an external memory, but they make different design trade-offs for \emph{addressing} (how memory is accessed) and \emph{sparsity} (how much memory is touched per step).
NTMs provide content-addressable reads/writes via dense attention over all memory slots; \vectur{} instead maintains a continuous head position on a tape and performs sparse gather/scatter updates to a small number of adjacent cells (via interpolation), which keeps per-step cost independent of tape length.
Dense access is expressive but \(O(N)\) in memory size and can induce diffuse ``memory blurring'' updates when many slots receive small writes; \vectur{} preserves untouched cells exactly by construction.
In Llama-style Transformer blocks, global content-based access is already available through self-attention; \vectur{} is intended to add an \emph{orthogonal} capability: cheap, iterative state manipulation with persistent scratchpad dynamics.

\begin{figure*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.18}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}p{0.20\textwidth}p{0.28\textwidth}p{0.28\textwidth}p{0.12\textwidth}@{}}
\toprule
\textbf{Feature} & \textbf{NTM (Graves)} & \textbf{\vectur{}} & \textbf{\vectur{} advantage} \\
\midrule
Addressing & Dense content-based attention over all slots & Local/location-based head movement on tape & \textbf{Yes} \\
Per-step complexity (vs.\ tape length \(N_T\)) & \(O(N_T)\) similarity + weighted sum & \(O(k)\) gather/scatter (independent of \(N_T\)) & \textbf{Yes} \\
Forward activation memory (unrolled \(T\) steps) & Stores dense weights \(\sim O(TN_T)\) & Stores sparse indices/weights \(\sim O(Tk)\) & \textbf{Yes} \\
State preservation away from head & Many slots updated slightly (drift/blurring risk) & Un-accessed cells are exactly unchanged & \textbf{Yes} \\
Content lookup in 1 step & Native (query by key) & Requires scanning via head movement (worst-case \(O(N_T)\) steps) & No \\
 Inductive bias & Random-access / associative recall & Sequential pointer machine / local algorithms \citep{vinyals2015pointer} & Depends \\
Compute allocation / halting & Typically fixed unroll or implicit stopping & Explicit learned halting via \(\kappa\) and gate \(g_t\) & \textbf{Yes} \\
Fit as a Transformer block & Redundant global attention inside block & Complements attention with iterative scratchpad dynamics & \textbf{Yes} \\
\bottomrule
\end{tabular}%
}
\caption{\textbf{NTM vs.\ \vectur{}.} NTMs \citep{graves2014ntm} provide dense, content-addressable memory access, while \vectur{} enforces sparse, local tape access with adaptive depth. The rightmost column highlights regimes where \vectur{} is especially advantageous as a computational block inside attention-based macro-architectures.}
\label{fig:ntm_vs_vectur}
\end{figure*}

\paragraph{Adaptive computation.}
Adaptive Computation Time (ACT) \citep{graves2016act} learns when to stop iterating, with later refinements such as PonderNet \citep{banino2021pondernet}.
Our learned gate \(g_t\) and \(\kappa\)-parameterization should be viewed as an ACT-style variant that provides a simple, input-conditioned control knob for effective depth; we do not position halting as the primary conceptual novelty.

\paragraph{Test-time training and online optimization.}
Recent work reframes sequence modeling as a form of \emph{online learning} or nested optimization carried out during inference, including end-to-end test-time training for long-context language modeling \citep{tandon2025ttt} and the MIRAS framework connecting attention, retention, and online optimization \citep{behrouz2025miras}.
This line of work motivates the viewpoint that ``System 2'' computation can be injected \emph{inside} a model by adding inner-loop dynamics as a composable block within the forward pass, rather than only via external deliberation or separate modules.

\paragraph{Randomized algorithms.}
Randomness can reduce expected runtime for verification and decision problems; canonical examples include Freivalds' randomized matrix product verification \citep{freivalds1977} and probabilistic primality testing \citep{miller1976,rabin1980}.
\vecstur{} is intended as a neural analogue that can exploit stochastic symbols during computation.

\section{VecTur: Vector Turing Machines}

\subsection{Vectorized machine state}
Given an input sequence \(x \in \R^{N \times d_x}\) (e.g., token embeddings), we define a \vectur{} block below. Note that for \vecstur{}, we additionally sample a sequence of stochastic symbols \(z \in \R^{N_z \times d_x}\) and set the tape length
\begin{equation}
N_T = N + N_z,
\end{equation}
so that each input symbol and each stochastic symbol can be addressed at least once. In our experiments we use \(N_z \approx N\) (so \(N_T \approx 2N\)).

We define the machine state at step \(t\) as a triple \((T_t,Q_t,I_t)\), where the tape \(T_t \in \R^{N_T \times d_T}\), the control state \(Q_t \in \R^{d_Q}\), and the head index
\[
I_t = (\boldsymbol{\theta}_t,\mathbf{w}_t) \in (S^1)^k \times \R^k
\]
are learned, differentiable quantities. (Here \(\boldsymbol{\theta}_t=(\theta_{t,1},\ldots,\theta_{t,k})\) parameterizes \(k\) head locations on the circle and \(\mathbf{w}_t=(w_{t,1},\ldots,w_{t,k})\) are the associated weights.)
We index tape positions by \(j\in\{0,1,\ldots,N_T-1\}\), and write \(T_t[j]\in\R^{d_T}\) for the \(j\)-th tape symbol.

The initial state is produced by learned maps \(\M_T,\M_Q,\M_I\) with parameters \(W\):

\begin{equation}
    T_0=\M_T(x;W), \quad Q_0=\M_Q(x;W), \quad I_0=\M_I(x;W),
\end{equation}

where $\M_T, \M_Q, \M_I$ can be any mapping using some parameters $W$.

\subsection{Sparse addressing (keeping I and J fixed)}
Define the following piecewise linear map \(E:S^1\to \R^{N_T}\) as

\begin{align*}
    % Define the segment index n
    n(\theta) &= \left\lfloor \frac{N_T \theta}{2\pi} \right\rfloor \\
    % Define the local interpolation parameter s
    s(\theta) &= \frac{N_T \theta}{2\pi} - \left\lfloor \frac{N_T \theta}{2\pi} \right\rfloor \\
    % The function E
    n^+(\theta) &= (n(\theta)+1)\bmod N_T \\
    E(\theta) &= (1 - s(\theta)) e_{n(\theta)} + s(\theta) e_{n^+(\theta)}
\end{align*}

We will write any \(I\in (S^1)^k\times\R^k\) as \(I=(\boldsymbol{\theta},\mathbf{w})\), and define the induced sparse tape-index weighting vector \(J(I)\in \R^{N_T}\) by

\begin{equation}
    J(I) = \sum_{i=1}^k w_i\,E(\theta_i).
\end{equation}

By construction, each \(E(\theta_i)\) is supported on at most two adjacent tape locations \(\{n(\theta_i),n^+(\theta_i)\}\), hence \(J(I)\) is supported on at most \(2k\) tape locations. Concretely, for each head atom \((\theta_i,w_i)\) define
\[
n_i := n(\theta_i),\qquad s_i := s(\theta_i),\qquad n_i^+ := (n_i + 1)\bmod N_T,
\]
so that \(E(\theta_i)=(1-s_i)e_{n_i}+s_ie_{n_i^+}\). This gives an implementation-friendly form: one can store \((n_i,n_i^+, (1-s_i)w_i, s_i w_i)\) for each \(i\) and never materialize the dense \(N_T\)-vector \(J(I)\).

\subsection{Read, transition, and halting}
We define the transition map \(\Delta\) that updates the tape, control state, and head index. First, we use the head index \(J(I_t)\) to read a single tape symbol \(S_t\in\R^{d_T}\):

\begin{equation}
    S_t = \sum_{j=0}^{N_T-1} \big(J(I_t)\big)_j\,T_t[j] \in \R^{d_T}.
\end{equation}
Equivalently, using the explicit \(2k\)-sparse form above,
\[
S_t
= \sum_{i=1}^k w_{t,i}\Big((1-s_{t,i})\,T_t[n_{t,i}] + s_{t,i}\,T_t[n_{t,i}^+]\Big),
\]
so \(S_t\) is computed using at most \(2k\) gathered tape vectors, and is piecewise linear in the tape (and linear in the interpolation weights away from the measure-zero segment boundaries induced by the floor operation).


Next, define a gate \(g_t\in(0,1)\) that controls the effective amount of computation and enables early stopping. We use a sigmoid gate,

\begin{equation}
    g_t
    = \sigma\!\left(\frac{-\kappa(x;W)\cdot t}{\max\!\big(1,\norm{Q_t-q_0}^2\big)}\right),
\end{equation}
where \(\sigma(u)=1/(1+e^{-u})\), \(\kappa(x;W)>0\) is a learned scalar per example, and \(q_0\in\R^{d_Q}\) is a learned halting target state. Intuitively, \(\kappa(x;W)\) acts as a \emph{decay-rate multiplier}: smaller \(\kappa(x;W)\) yields a slower decay in \(t\) (more effective steps), while larger \(\kappa(x;W)\) yields a faster decay (fewer effective steps). The factor \(\|Q_t-q_0\|\) encourages the dynamics to become stationary near the target.

We update the tape, control state, and head index using learned transition maps \(\Delta_T,\Delta_Q,\Delta_\theta,\Delta_w\). Let
\[
U_t := \Delta_T(S_t,Q_t;W)\in\R^{d_T}.
\]
Then the update equations are

\begin{align}
    T_{t+1}[j] &= T_t[j] + g_t\,\big(J(I_t)\big)_j\,U_t \qquad \text{for } j\in\{0,\ldots,N_T-1\}, \label{eq:vectur_tape_update}\\
    Q_{t+1} &= Q_t + g_t\,\Delta_Q(S_t,Q_t;W),\\
    \boldsymbol{\theta}_{t+1} &= \big(\boldsymbol{\theta}_t + g_t\,\Delta_\theta(S_t,Q_t,\boldsymbol{\theta}_t;W)\big)\bmod 2\pi,\\
    \mathbf{w}_{t+1} &= \mathbf{w}_t + g_t\,\Delta_w(S_t,Q_t,\mathbf{w}_t;W),\\
    I_{t+1} &= (\boldsymbol{\theta}_{t+1},\mathbf{w}_{t+1}).\label{eq:vectur_head_update}
\end{align}

Equation~\eqref{eq:vectur_tape_update} makes the sparsity explicit: since \(\big(J(I_t)\big)_j=0\) for all but at most \(2k\) locations, only \(O(2k)\) tape vectors are updated per step. In an efficient implementation, \eqref{eq:vectur_tape_update} is executed as a scatter-add into those \(2k\) indices (and \(S_t\) is computed as a gather + weighted sum).

The transition maps have the following types:
\begin{align*}
\Delta_T &:\R^{d_T}\times\R^{d_Q}\to\R^{d_T},\\
\Delta_Q &:\R^{d_T}\times\R^{d_Q}\to\R^{d_Q},\\
\Delta_\theta &:\R^{d_T}\times\R^{d_Q}\times(S^1)^k\to\R^k,\\
\Delta_w &:\R^{d_T}\times\R^{d_Q}\times\R^k\to\R^k.
\end{align*}
The \(\bmod 2\pi\) in \eqref{eq:vectur_head_update} ensures the head angles represent elements of \(S^1\) (equivalently, \(\Delta_\theta\) may be chosen \(2\pi\)-periodic in each component).
With sparse gather/scatter, one step costs \(O(k(d_T+d_Q))\) time and \(O(k(d_T+d_Q))\) working memory, plus the cost of evaluating the small transition networks.

\paragraph{Early stopping and block output.}
Fix a maximum unroll \(T_{\max}\in\N\) and a threshold \(\varepsilon>0\). We run the transition until either \(t=T_{\max}\) or the gate becomes negligible,
\[
T(x) = \min\{t\in\{0,\ldots,T_{\max}-1\} : g_t < \varepsilon\},
\]
with the convention \(T(x)=T_{\max}\) if the set is empty. Concretely, we check \(g_t<\varepsilon\) at the beginning of step \(t\); if it holds, we stop and return \(T_t\). Otherwise, we apply the transition to produce \(T_{t+1}\) and continue.
We define the \vectur{} block output as the final tape
\[
V(x) := T_{T(x)} \in \R^{N_T\times d_T}.
\]
In downstream architectures (e.g., Llama-style models), any required reshaping or projection of \(V(x)\) is handled outside the \vectur{} block.

\paragraph{Differentiability and efficient backpropagation.}
All operations inside each step are differentiable with respect to the tape values and the transition parameters, except at the measure-zero boundaries induced by the floor/mod operations inside \(n(\theta)\). In practice, we implement reading and writing via gather/scatter on the at-most-\(2k\) active indices, which is efficient and supports backpropagation through the unrolled computation.
Early stopping introduces a discrete dependence on the stopping time \(T(x)\); a standard choice is to stop the forward pass when \(g_t<\varepsilon\) and treat the control-flow decision as non-differentiable, while gradients still flow through all executed steps (alternatively, one can always run for \(T_{\max}\) steps and rely on the multiplicative \(g_t\) factors to effectively mask later updates).

\paragraph{Concrete parameterization (used in experiments).}
We instantiate the maps \(\M_T, \M_Q, \M_I\) as linear projections, and the transition maps \(\Delta_T, \Delta_Q,\Delta_w\) as two-layer MLPs with expansion factor 4.
Specifically, we project tape symbols position-wise,
\[
\M_T(x;W) = x W_T,\quad \M_T(z;W)= z W_T,
\]
and define \(\M_Q,\M_I\) as learnable linear maps that collapse the sequence to the required shapes. Writing
\[
\mathrm{vec}(x) := [x[1];x[2];\cdots;x[N]]\in\R^{N d_x},
\]
we set
\[
\M_Q(x;W) = W_Q\,\mathrm{vec}(x)\in\R^{d_Q},\qquad
\M_I(x;W)=\big(\boldsymbol{\theta}_0,\mathbf{w}_0\big),
\]
with
\[
\boldsymbol{\theta}_0 = (W_\theta\,\mathrm{vec}(x))\bmod 2\pi \in (S^1)^k,\qquad
\mathbf{w}_0 = W_w\,\mathrm{vec}(x)\in\R^k.
\]
No constraint is imposed on \(\mathbf{w}_0\); weights may be any real numbers.

We choose \(\kappa(x;W)\) as a two-layer MLP (expansion factor 4) with a positivity constraint so that \(\kappa(x;W) > 0\).
For \(\Delta_\theta\), we parameterize periodicity by feeding \(\sin(\boldsymbol{\theta}_t)\) and \(\cos(\boldsymbol{\theta}_t)\) into an MLP; concretely,
\[
\Delta_\theta(S_t,Q_t,\boldsymbol{\theta}_t;W)
= \mathrm{MLP}_\theta\big([S_t,Q_t,\sin(\boldsymbol{\theta}_t),\cos(\boldsymbol{\theta}_t)]\big)\in\R^k.
\]

\paragraph{Algorithm (forward pass).}
Given \((T_0,Q_0,I_0)\), we iterate for \(t=0,1,\ldots,T_{\max}-1\):
\begin{enumerate}[leftmargin=*,nosep]
    \item compute \((n_{t,i},s_{t,i},n_{t,i}^+)_{i=1}^k\) from \(\boldsymbol{\theta}_t\) via the definitions above;
    \item read \(S_t\) as a \(2k\)-term weighted sum of gathered tape vectors;
    \item compute \(g_t\); if \(g_t<\varepsilon\), stop early and return \(T_t\);
    \item update \(Q_{t+1}\) and update \((\boldsymbol{\theta}_{t+1},\mathbf{w}_{t+1})\);
    \item write by scatter-adding into the at-most-\(2k\) tape locations \(\{n_{t,i},n_{t,i}^+\}_{i=1}^k\) according to \eqref{eq:vectur_tape_update};
\end{enumerate}
We return \(V(x)=T_{T(x)}\).

\section{VecTur Blocks inside Llama-style Models}
\subsection{Macro architecture}
We adopt a standard decoder-only transformer macro architecture (token embeddings, positional encoding \citep{su2021roformer}, residual blocks, and an LM head) following Llama-family designs \citep{touvron2023llama2,dubey2024llama3}.
We then vary the \emph{block} inside each residual layer while keeping parameter count and FLOPs roughly matched.
This ``block as inner loop'' framing is inspired by recent work that integrates deliberate, multi-step computation into the forward pass via online learning or test-time adaptation, notably TTT-style test-time training \citep{tandon2025ttt} and MIRAS-style online optimization views of sequence models \citep{behrouz2025miras}.
In that spirit, we view \vectur{} as an explicit, constrained ``System 2'' transition system embedded as a sub-layer inside a ``System 1'' decoder, rather than as a standalone memory system that replaces the macro architecture.

\begin{figure}[t]
\centering
\fbox{\rule{0pt}{1.3in}\rule{0.93\linewidth}{0pt}}
\caption{\textbf{Placeholder.} Block-swap experiment: a fixed Llama-style macro architecture where the per-layer computational block is one of \{Attention, LSTM, NTM/DNC, \vectur{}, \vecstur{}\}.}
\label{fig:blockswap}
\end{figure}

\subsection{Compared blocks}
We compare the following blocks:
\begin{itemize}[leftmargin=*,nosep]
  \item \textbf{Attention block}: multi-head self-attention \citep{vaswani2017attention} + SwiGLU MLP \citep{shazeer2020gluvariants}.
  \item \textbf{LSTM block}: a gated recurrent update applied over the sequence, wrapped with residual connections \citep{hochreiter1997lstm}.
  \item \textbf{External-memory block}: an NTM/DNC-style controller with differentiable read/write heads \citep{graves2014ntm,graves2016dnc}.
  \item \textbf{\vectur{} block}: the \vectur{} transition unrolled for \(T_{\max}\) steps with learned halting \(\kappa\).
  \item \textbf{\vecstur{} block}: \vectur{} with stochastic symbols \(z\).
\end{itemize}

\section{Evaluation Benchmarks}
\subsection{Reasoning and knowledge}
We evaluate few-shot or fine-tuned performance on:
\begin{itemize}[leftmargin=*,nosep]
  \item \textbf{GSM8K} \citep{cobbe2021gsm8k} (grade-school math; exact-match accuracy),
  \item \textbf{ARC} \citep{clark2018arc} (AI2 reasoning challenge; accuracy),
  \item \textbf{HellaSwag} \citep{zellers2019hellaswag} (commonsense completion; accuracy).
\end{itemize}

\subsection{Language modeling}
We evaluate next-token prediction on \textbf{WikiText-103} \citep{merity2016wikitext} using perplexity.

\section{CompGen: Complexity-Stratified Program Generation}
\subsection{Task format}
\compgen{} consists of short Python programs \(p\) paired with inputs \(u\) and outputs \(p(u)\).
Each instance is labeled with a target complexity class \((T(n),S(n))\) in terms of input size \(n\) \citep{sipser2012toc}.
Programs are generated from templates with controlled loop structure, recursion depth, and memory allocation patterns.
We view \compgen{} as a utility dataset in the tradition of synthetic algorithmic benchmarks, complementary to the CLRS Algorithmic Reasoning Benchmark \citep{velickovic2022clrs}.

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Class} & \textbf{Example family} & \textbf{Notes} \\
\midrule
\(O(n), O(1)\) & scan / reduce & single pass \\
\(O(n), O(n)\) & prefix sums & linear auxiliary array \\
\(O(n\log n), O(1)\) & sort-then-scan & comparison sorting \\
\(O(n^2), O(1)\) & nested-loop count & quadratic time \\
\(O(n^2), O(n)\) & DP table strip & quadratic time, linear space \\
\bottomrule
\end{tabular}
\caption{\textbf{Placeholder.} \compgen{} program families and intended \((T(n),S(n))\) buckets.}
\label{tab:compgen}
\end{table}

\subsection{Generalization protocol}
We train on \(n \in [n_{\min}, n_{\text{train}}]\) and evaluate on larger \(n \in (n_{\text{train}}, n_{\text{test}}]\) to measure extrapolation.
We report accuracy as a function of \(n\) and correlate effective compute (average unroll steps) with complexity class.

\section{Randomized Computation Suite}
We include tasks where access to randomness enables provable or empirical speedups:
\begin{itemize}[leftmargin=*,nosep]
  \item \textbf{Matrix product verification} (Freivalds) \citep{freivalds1977}: verify \(AB=C\) faster than multiplication.
  \item \textbf{Probabilistic primality testing} (Miller--Rabin) \citep{miller1976,rabin1980}: decide primality with bounded error.
\end{itemize}
\vecstur{} receives stochastic symbols \(z\) and learns to leverage them to reduce expected compute (as reflected by learned \(\kappa\) and early halting).

\section{Experimental Setup}
\paragraph{Model sizes.}
We instantiate models at \(\sim\)110M, 350M, and 1.3B parameters (placeholder sizes) with matched embedding width and layer count across blocks.

\paragraph{Blocks and controlled comparisons.}
Unless otherwise stated, we run the same experiment for each block in Section~3 (Attention, LSTM, NTM/DNC, \vectur{}, \vecstur{}), holding the decoder-only macro architecture fixed and matching parameter count and training budget as closely as possible.

\paragraph{Experimental protocols (run per block).}
We use three complementary training/evaluation protocols:
\begin{enumerate}[leftmargin=*,nosep]
  \item \textbf{Language pretraining \(\rightarrow\) downstream evaluation.}
  We pretrain on \textbf{FineWeb} (general web text), then evaluate on \textbf{GSM8K} \citep{cobbe2021gsm8k}, \textbf{ARC} \citep{clark2018arc}, \textbf{HellaSwag} \citep{zellers2019hellaswag}, and \textbf{WikiText-103} \citep{merity2016wikitext}.
  (Table~\ref{tab:benchmarks}.)

  \item \textbf{Algorithmic transfer between CLRS and \compgen{}.}
  (a) \textbf{Train on CLRS} \citep{velickovic2022clrs} and evaluate on \compgen{} under three regimes: \emph{zero-shot} (no \compgen{} training), \emph{few-shot} (in-context demonstrations at test time), and \emph{fine-tune} (supervised adaptation on \compgen{} train).
  (b) \textbf{Train on \compgen{}} and evaluate on a held-out \compgen{} split (including out-of-distribution generalization across input sizes \(n\) per Section~5.2).
  (Figure~\ref{fig:compgen}.)

  \item \textbf{In-domain CLRS generalization.}
  We train on CLRS \citep{velickovic2022clrs} and evaluate on a held-out CLRS split (standard in-distribution generalization across graphs/sizes/instances).
  (Reported alongside other algorithmic results; placeholder in this draft.)
\end{enumerate}

\paragraph{Optimization and budgets.}
Within each protocol, we use identical optimizers, learning rate schedules, and token/step budgets across blocks (to isolate architectural effects).

\paragraph{Compute control.}
For \vectur{}/\vecstur{} we set a maximum unroll \(T_{\max}\) and learn \(\kappa(x;W)\) to modulate effective steps.
We report both task performance and measured compute (average unroll steps per token).

\section{Results (Illustrative Placeholders)}
\paragraph{Important note.}
The tables below contain \textbf{Lorem ipsum placeholder entries} showing the intended presentation format \emph{and} explicitly recording the train/test split for each experiment protocol.
Replace these placeholders with measured metrics.

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.08}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Block (model)} & \textbf{Train set} & \textbf{GSM8K (test)} & \textbf{ARC (test)} & \textbf{HellaSwag (test)} & \textbf{WikiText-103 (test)} \\
\midrule
Attention & FineWeb & Lorem & Ipsum & Dolor & Sit \\
LSTM & FineWeb & Amet & Consectetur & Adipiscing & Elit \\
NTM/DNC & FineWeb & Sed & Do & Eiusmod & Tempor \\
\vectur{} & FineWeb & Incididunt & Ut & Labore & Et \\
\vecstur{} & FineWeb & Magna & Aliqua & Ut & Enim \\
\bottomrule
\end{tabular}%
}
\caption{\textbf{Placeholder (Protocol 1).} Language pretraining on FineWeb, evaluated on downstream benchmarks. Entries are Lorem ipsum placeholders.}
\label{tab:benchmarks}
\end{table*}

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.08}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Block} & \textbf{Train} & \textbf{Test} & \textbf{Result} \\
\midrule
Attention & CLRS & \compgen{} (zero-shot) & Lorem ipsum \\
Attention & CLRS & \compgen{} (few-shot) & Dolor sit \\
Attention & CLRS & \compgen{} (fine-tune) & Amet consectetur \\
LSTM & CLRS & \compgen{} (zero-shot) & Adipiscing elit \\
LSTM & CLRS & \compgen{} (few-shot) & Sed do \\
LSTM & CLRS & \compgen{} (fine-tune) & Eiusmod tempor \\
NTM/DNC & CLRS & \compgen{} (zero-shot) & Incididunt ut \\
NTM/DNC & CLRS & \compgen{} (few-shot) & Labore et \\
NTM/DNC & CLRS & \compgen{} (fine-tune) & Magna aliqua \\
\vectur{} & CLRS & \compgen{} (zero-shot) & Ut enim \\
\vectur{} & CLRS & \compgen{} (few-shot) & Ad minim \\
\vectur{} & CLRS & \compgen{} (fine-tune) & Veniam quis \\
\vecstur{} & CLRS & \compgen{} (zero-shot) & Nostrud exercitation \\
\vecstur{} & CLRS & \compgen{} (few-shot) & Ullamco laboris \\
\vecstur{} & CLRS & \compgen{} (fine-tune) & Nisi ut \\
\bottomrule
\end{tabular}
\caption{\textbf{Placeholder (Protocol 2a).} Train on CLRS, test on \compgen{} under zero-shot / few-shot / fine-tune adaptation regimes. Results are placeholders.}
\label{tab:clrs_to_compgen}
\end{table}

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.08}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Block} & \textbf{Train} & \textbf{Test} & \textbf{Result} \\
\midrule
Attention & \compgen{} (train) & \compgen{} (held-out) & Lorem ipsum \\
LSTM & \compgen{} (train) & \compgen{} (held-out) & Dolor sit \\
NTM/DNC & \compgen{} (train) & \compgen{} (held-out) & Amet consectetur \\
\vectur{} & \compgen{} (train) & \compgen{} (held-out) & Adipiscing elit \\
\vecstur{} & \compgen{} (train) & \compgen{} (held-out) & Sed do \\
\bottomrule
\end{tabular}
\caption{\textbf{Placeholder (Protocol 2b).} Train on \compgen{}, test on held-out \compgen{} (including extrapolation across larger \(n\)). Results are placeholders.}
\label{tab:compgen_holdout}
\end{table}

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.08}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Block} & \textbf{Train} & \textbf{Test} & \textbf{Result} \\
\midrule
Attention & CLRS (train) & CLRS (held-out) & Lorem ipsum \\
LSTM & CLRS (train) & CLRS (held-out) & Dolor sit \\
NTM/DNC & CLRS (train) & CLRS (held-out) & Amet consectetur \\
\vectur{} & CLRS (train) & CLRS (held-out) & Adipiscing elit \\
\vecstur{} & CLRS (train) & CLRS (held-out) & Sed do \\
\bottomrule
\end{tabular}
\caption{\textbf{Placeholder (Protocol 3).} Train on CLRS and evaluate on a held-out CLRS split. Results are placeholders.}
\label{tab:clrs_holdout}
\end{table}

\begin{figure}[t]
\centering
\fbox{\rule{0pt}{1.5in}\rule{0.93\linewidth}{0pt}}
\caption{\textbf{Placeholder.} \compgen{} extrapolation: accuracy vs.\ input size \(n\), showing how blocks degrade with larger \(n\) and how \vectur{} modulates effective steps via learned \(\kappa\).}
\label{fig:compgen}
\end{figure}

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Block} & \textbf{Train set} & \textbf{Test set} & \textbf{Result} \\
\midrule
\vectur{} & Randomized suite (train) & Freivalds / Miller--Rabin (test) & Lorem ipsum \\
\vecstur{} & Randomized suite (train) & Freivalds / Miller--Rabin (test) & Dolor sit amet \\
\bottomrule
\end{tabular}
\caption{\textbf{Placeholder.} Randomized computation suite: train/test bookkeeping with placeholder results.}
\label{tab:random}
\end{table}

\section{Discussion}
These illustrative results suggest \vectur{} provides a useful inductive bias for tasks requiring iterative computation and length extrapolation, while remaining compatible with modern LLM macro architectures.
Importantly, the strongest claims in this paper are \emph{not} that differentiable Turing machines are new, but that (i) enforcing strictly local sparse access yields a practical, non-blurring pointer-machine-style block, (ii) treating such a machine as a composable Transformer sub-layer is a strong systems contribution, and (iii) \vecstur{} highlights a comparatively underexplored angle: learning to exploit randomness as a computational resource in randomized-algorithm tasks.
\vecstur{} further improves performance on tasks where randomized strategies are advantageous.

\section{Limitations and Future Work}
This draft omits implementation details (e.g., the exact \(\mathrm{Sparse}(\cdot)\) operator, stability constraints, and efficient kernels) and uses illustrative results.
Future work should (i) benchmark on longer-context settings, (ii) analyze failure modes of learned halting \(\kappa\), and (iii) evaluate robustness across different data mixtures and training budgets.

\subsection{Future Work: Mechanistic Interpretability}
\label{sec:future_mi}
\vectur{} is unusually well-suited for mechanistic interpretability \citep{olah2020zoomin,elhage2021framework} because its learned dynamics are constrained to resemble an explicit Turing-style transition system: a finite-dimensional control state \(Q_t\), a tape \(T_t\), and a small number of heads with sparse, local read/write effects.
This structure encourages explanations in terms of \emph{state machines} and \emph{pointer-based algorithms} (e.g., ``scan until condition,'' ``increment counter,'' ``copy span,'' ``simulate update rule''), rather than opaque global attention patterns.

A promising direction is to \emph{disassemble} trained \vectur{} blocks into more directly inspectable artifacts.
For example, one can post-hoc discretize head locations, identify stable control states, and summarize the transition maps \(\Delta\) as a symbolic program or a finite set of guarded update rules; such representations can then be \emph{transpiled} into executable code, enabling unit tests, counterfactual interventions, and formal analysis of the implied algorithm.

Finally, \vectur{} may serve as an interpretable \emph{surrogate} for black-box sequence models.
Analogous to knowledge distillation \citep{hinton2015distilling,romero2015fitnets}, one can perform \emph{cross-distillation}: train a \vectur{} model to mimic the input--output behavior (and, when available, internal activations) of an existing architecture, with the goal that the learned tape-and-control dynamics provide a concrete hypothesis for the black box's implicit Turing-style computation.
Such surrogates could support ``algorithmic guessing''---extracting candidate programs from the \vectur{} dynamics---followed by validation against the teacher via targeted probes and adversarial test cases.

\section*{Acknowledgments}
\emph{Placeholder.}

\begin{thebibliography}{99}

\bibitem[Turing(1936)]{turing1936}
Alan~M. Turing.
\newblock On computable numbers, with an application to the {E}ntscheidungsproblem.
\newblock \emph{Proceedings of the London Mathematical Society}, 1936.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 1997.

\bibitem[Vaswani et~al.(2017)]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Graves et~al.(2014)]{graves2014ntm}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural {T}uring machines.
\newblock \emph{arXiv:1410.5401}, 2014.

\bibitem[Graves et~al.(2016)]{graves2016dnc}
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi{\'n}ska, Sergio G{\'o}mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adri{\`a} Puigdom{\`e}nech Badia, Karl~Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 2016.

\bibitem[Graves(2016)]{graves2016act}
Alex Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock \emph{arXiv:1603.08983}, 2016.

\bibitem[Touvron et~al.(2023)]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et~al.
\newblock {Llama 2}: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv:2307.09288}, 2023.

\bibitem[Dubey et~al.(2024)]{dubey2024llama3}
Abhimanyu Dubey et~al.
\newblock The {Llama 3} herd of models.
\newblock \emph{arXiv preprint}, 2024.

\bibitem[Cobbe et~al.(2021)]{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukas Kaiser, Matthias Plappert, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv:2110.14168}, 2021.

\bibitem[Clark et~al.(2018)]{clark2018arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try {ARC}, the {AI2} reasoning challenge.
\newblock \emph{arXiv:1803.05457}, 2018.

\bibitem[Zellers et~al.(2019)]{zellers2019hellaswag}
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi.
\newblock {HellaSwag}: Can a machine really finish your sentence?
\newblock In \emph{ACL}, 2019.

\bibitem[Merity et~al.(2016)]{merity2016wikitext}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv:1609.07843}, 2016.
\newblock (Introduces the WikiText-103 benchmark.)

\bibitem[Freivalds(1977)]{freivalds1977}
Raimund Freivalds.
\newblock Probabilistic machines can use less running time.
\newblock In \emph{IFIP Congress}, 1977.

\bibitem[Miller(1976)]{miller1976}
Gary~L. Miller.
\newblock Riemann's hypothesis and tests for primality.
\newblock \emph{Journal of Computer and System Sciences}, 1976.

\bibitem[Rabin(1980)]{rabin1980}
Michael~O. Rabin.
\newblock Probabilistic algorithm for testing primality.
\newblock \emph{Journal of Number Theory}, 1980.

\bibitem[Brown et~al.(2020)]{brown2020gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Dehghani et~al.(2019)]{dehghani2019universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz Kaiser.
\newblock Universal transformers.
\newblock In \emph{ICLR}, 2019.

\bibitem[Banino et~al.(2021)]{banino2021pondernet}
Andrea Banino, Jan Balaguer, Charles Blundell, and Andrew Zisserman.
\newblock PonderNet: Learning to ponder.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Behrouz et~al.(2025)]{behrouz2025miras}
Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni.
\newblock It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization.
\newblock \emph{arXiv:2504.13173}, 2025.

\bibitem[Elhage et~al.(2021)]{elhage2021framework}
Nelson Elhage, Sam~S. McCandlish, Catherine Olsson, Christopher Henighan, Nicholas Joseph, Ben Mann, Seth Kaplan, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits (Anthropic)}, 2021.

\bibitem[Kaiser and Sutskever(2016)]{kaiser2016neuralgpu}
{\L}ukasz Kaiser and Ilya Sutskever.
\newblock Neural {GPU}s learn algorithms.
\newblock In \emph{ICLR}, 2016.

\bibitem[Olah et~al.(2020)]{olah2020zoomin}
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and the OpenAI Clarity team.
\newblock Zoom in: An introduction to circuits.
\newblock \emph{Distill}, 2020.

\bibitem[Press et~al.(2022)]{press2022trainshort}
Ofir Press, Noah~A. Smith, and Mike Lewis.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock In \emph{ICLR}, 2022.

\bibitem[Hinton et~al.(2015)]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv:1503.02531}, 2015.

\bibitem[Romero et~al.(2015)]{romero2015fitnets}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.
\newblock FitNets: Hints for thin deep nets.
\newblock In \emph{ICLR}, 2015.

\bibitem[Shazeer(2020)]{shazeer2020gluvariants}
Noam Shazeer.
\newblock {GLU} variants improve transformer.
\newblock \emph{arXiv:2002.05202}, 2020.

\bibitem[Sipser(2012)]{sipser2012toc}
Michael Sipser.
\newblock \emph{Introduction to the Theory of Computation}.
\newblock Cengage Learning, 3rd edition, 2012.

\bibitem[Su et~al.(2021)]{su2021roformer}
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.
\newblock RoFormer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv:2104.09864}, 2021.

\bibitem[Vinyals et~al.(2015)]{vinyals2015pointer}
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
\newblock Pointer networks.
\newblock In \emph{NeurIPS}, 2015.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2022)]{velickovic2022clrs}
Petar Veli{\v{c}}kovi{\'c}, Adri{\`a} Puigdom{\`e}nech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Daswani, Raia Hadsell, and Charles Blundell.
\newblock The CLRS Algorithmic Reasoning Benchmark.
\newblock \emph{arXiv preprint arXiv:2205.15659}, 2022.

\bibitem[Tandon et~al.(2025)]{tandon2025ttt}
Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel R{\o}d, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, and Yu Sun.
\newblock End-to-End Test-Time Training for Long Context.
\newblock \emph{arXiv:2512.23675}, 2025.

\end{thebibliography}

\end{document}

